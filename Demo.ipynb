{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Convolution Neural Network with Trained Word2Vec Embeddings for Part-Of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural language processing in artificial intelligence has come very far, but still has a ways to go.\n",
    "### Are neural networks worth exploring for natural language processing in AI?\n",
    "### Let's find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Required Files: latinModel, englishModel, ttokenizer.json, ttokenizerlatin.json, wtokenizer.json, wtokenizerlatin.json*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________\n",
    "### Step 1: Run the following three cells to import the required packages and prepare some information for the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This cell imports all of the required Tensorflow/Keras and Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The networks pad input sentences with zeros to create uniform length, but this method inflates the accuracy rate used for evaluation. A masked accuracy is therefore used for more precise evaluation. \n",
    "##### This cell defines the function for the masked accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_masked(y_true, y_pred):\n",
    "    y_true_class = K.argmax(y_true, axis=-1)\n",
    "    y_pred_class = K.argmax(y_pred, axis=-1)\n",
    "\n",
    "    ignore_mask = K.cast(K.not_equal(y_true_class, 0), 'int32')\n",
    "    matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This cell creates a Python dictionary of the various parts of speech (tags) that may be found in a sentence for English and Latin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagdictenglish = {\n",
    "    \"conj\": \"Conjunction\",\n",
    "    \".\": \"Punctuation\",\n",
    "    \"propn\": \"Proper Noun\",\n",
    "    \"num\": \"Numeral\",\n",
    "    \"adv\": \"Adverb\",\n",
    "    \"verb\": \"Verb\",\n",
    "    \"noun\": \"Noun\",\n",
    "    \"pron\": \"Pronoun\",\n",
    "    \"adj\": \"Adjective\",\n",
    "    \"part\": \"Participle\",\n",
    "    \"det\": \"Determiner\",\n",
    "    \"x\": \"Other\",\n",
    "    \"adp\": \"Adposition\",\n",
    "}\n",
    "\n",
    "tagdictlatin = {\n",
    "    \"propn\": \"Proper Noun\",\n",
    "    \"adv\": \"Adverb\",\n",
    "    \"x\": \"Other\",\n",
    "    \"intj\": \"Interjection\",\n",
    "    \"cconj\": \"Coordinating Conjunction\",\n",
    "    \"punct\": \"Punctuation\",\n",
    "    \"det\": \"Determiner\",\n",
    "    \"adj\": \"Adjective\",\n",
    "    \"pron\": \"Pronoun\",\n",
    "    \"sconj\": \"Subordinating Conjunction\",\n",
    "    \"num\": \"Numeral\",\n",
    "    \"aux\": \"Auxiliary Verb\",\n",
    "    \"noun\": \"Noun\",\n",
    "    \"adp\": \"Adposition\",\n",
    "    \"verb\": \"Verb\",\n",
    "    \"part\": \"Particle\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________\n",
    "### Step 2: Run the next three cells to load the models and their Tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This cell downloads the files for the models and Tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.cs.mtsu.edu/~mam2hu/englishModel.zip\n",
    "!unzip englishModel.zip\n",
    "!wget https://www.cs.mtsu.edu/~mam2hu/latinModel.zip\n",
    "!unzip latinModel.zip\n",
    "!wget https://www.cs.mtsu.edu/~mam2hu/ttokenizer.json\n",
    "!wget https://www.cs.mtsu.edu/~mam2hu/wtokenizer.json\n",
    "!wget https://www.cs.mtsu.edu/~mam2hu/ttokenizerlatin.json\n",
    "!wget https://www.cs.mtsu.edu/~mam2hu/wtokenizerlatin.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This cell loads the two neural networks, one for English and one for Latin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_english_model_tf = tf.keras.models.load_model('englishModel',custom_objects={'accuracy_masked':accuracy_masked})\n",
    "loaded_latin_model_tf = tf.keras.models.load_model('latinModel', custom_objects={'accuracy_masked':accuracy_masked})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Tokenizer import from Keras allows text to be converted into a vector that is used by the network. The text is separated by punctuation and space into a sequence of words, which is then split into lists of tokens. \n",
    "##### This cell loads the Tokenizers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wtokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    word_tokenizer_english = keras.preprocessing.text.tokenizer_from_json(data)\n",
    "    \n",
    "with open('ttokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tag_tokenizer_english = keras.preprocessing.text.tokenizer_from_json(data)\n",
    "    \n",
    "with open('wtokenizerlatin.json') as f:\n",
    "    data = json.load(f)\n",
    "    word_tokenizer_latin = keras.preprocessing.text.tokenizer_from_json(data)\n",
    "    \n",
    "with open('ttokenizerlatin.json') as f:\n",
    "    data = json.load(f)\n",
    "    tag_tokenizer_latin = keras.preprocessing.text.tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________\n",
    "### Step 3: Run the next two cells to define the Part-of-Speech-Tagging Functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following two cells define the functions used by the networks to preprocess the input sentences, make a prediction for the parts of speech, and display the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_prediction_english(text):\n",
    "    \n",
    "    # Preprocess data by tokenizing input sentences and padding with zeros\n",
    "    text = [word_tokenize(text)]\n",
    "    text_encoded = word_tokenizer_english.texts_to_sequences(text) \n",
    "    text_padded = pad_sequences(text_encoded, maxlen=50, padding='pre', truncating='post')\n",
    "    \n",
    "    # Make a prediction \n",
    "    ynew = np.argmax(loaded_english_model_tf.predict(text_padded), axis=-1)\n",
    "    prediction = ynew[0]\n",
    "    \n",
    "    # Trim leading zeros\n",
    "    prediction = np.trim_zeros(prediction)\n",
    "    \n",
    "    # Decode the prediction\n",
    "    decoded = tag_tokenizer_english.sequences_to_texts([prediction])\n",
    "    decoded = word_tokenize(decoded[0])\n",
    "    decoded = [tagdictenglish[tag] for tag in decoded]\n",
    "    \n",
    "    # Display the inputs and predicted outputs\n",
    "    print(\"      Sentence= %s\\nPredicted Tags= %s\" % (text, decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_prediction_latin(text):\n",
    "    \n",
    "    #Preprocess data by tokenizing input sentences and padding with zeros\n",
    "    text = [word_tokenize(text)]\n",
    "    text_encoded = word_tokenizer_latin.texts_to_sequences(text) \n",
    "    text_padded = pad_sequences(text_encoded, maxlen=50, padding='pre', truncating='post')\n",
    "    \n",
    "    # Make a prediction\n",
    "    ynew = np.argmax(loaded_latin_model_tf.predict(text_padded), axis=-1)\n",
    "    prediction = ynew[0]\n",
    "    \n",
    "    # Trim leading zeros\n",
    "    prediction = np.trim_zeros(prediction)\n",
    "    \n",
    "    # Decode the prediction\n",
    "    decoded = tag_tokenizer_latin.sequences_to_texts([prediction])\n",
    "    decoded = word_tokenize(decoded[0])\n",
    "    decoded = [tagdictlatin[tag] for tag in decoded]\n",
    "    \n",
    "    # Display the inputs and predicted outputs\n",
    "    print(\"      Sentence= %s\\nPredicted Tags= %s\" % (text, decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "### Step 4: See the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create your own English and Latin sentences and use them to replace the sample sentences below, or just leave the sample sentences if you aren't feeling creative, then run the cell to see the parts of speech:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Note: Be sure to leave the quotation marks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = \"The red cat took a very long walk along the winding river.\"  \n",
    "output_prediction_english(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin = \"Forsan et haec olim meminisse iuvabit.\"\n",
    "output_prediction_latin(latin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "### You have now successfully used neural networks to tag sentences for English and Latin! The network model can be trained to tag sentences in any language as long as an extensive and tagged dataset is used during training.\n",
    "\n",
    "### This demo shows that natural language processing and neural networks have a promising future together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
