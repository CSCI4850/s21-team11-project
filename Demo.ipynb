{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Nerual Network with Trained Word2Vec Embeddings for Part-Of-Speech Tagging - Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary Files: latinModel, englishModel, ttokenizer.json, ttokenizerlatin.json, wtokenizer.json, wtokenizerlatin.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________\n",
    "### Step 1: Run the following three cells to prepare some data the model will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_masked(y_true, y_pred):\n",
    "    y_true_class = K.argmax(y_true, axis=-1)\n",
    "    y_pred_class = K.argmax(y_pred, axis=-1)\n",
    "\n",
    "    ignore_mask = K.cast(K.not_equal(y_true_class, 0), 'int32')\n",
    "    matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagdictenglish = {\n",
    "    \"conj\": \"Conjunction\",\n",
    "    \".\": \"Punctuation\",\n",
    "    \"propn\": \"Proper Noun\",\n",
    "    \"num\": \"Numeral\",\n",
    "    \"adv\": \"Adverb\",\n",
    "    \"verb\": \"Verb\",\n",
    "    \"noun\": \"Noun\",\n",
    "    \"pron\": \"Pronoun\",\n",
    "    \"adj\": \"Adjective\",\n",
    "    \"part\": \"Particle\",\n",
    "    \"det\": \"Determiner\",\n",
    "    \"x\": \"Other\",\n",
    "    \"adp\": \"Adposition\",\n",
    "}\n",
    "\n",
    "tagdictlatin = {\n",
    "    \"propn\": \"Proper Noun\",\n",
    "    \"adv\": \"Adverb\",\n",
    "    \"x\": \"Other\",\n",
    "    \"intj\": \"Interjection\",\n",
    "    \"cconj\": \"Coordinating Conjunction\",\n",
    "    \"punct\": \"Punctuation\",\n",
    "    \"det\": \"Determiner\",\n",
    "    \"adj\": \"Adjective\",\n",
    "    \"pron\": \"Pronoun\",\n",
    "    \"sconj\": \"Subordinating Conjunction\",\n",
    "    \"num\": \"Numeral\",\n",
    "    \"aux\": \"Auxiliary Verb\",\n",
    "    \"noun\": \"Noun\",\n",
    "    \"adp\": \"Adposition\",\n",
    "    \"verb\": \"Verb\",\n",
    "    \"part\": \"Particle\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________\n",
    "### Step 2: Run the next two cells to load up the models and their Tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_english_model_tf = tf.keras.models.load_model('englishModel',custom_objects={'accuracy_masked':accuracy_masked})\n",
    "loaded_latin_model_tf = tf.keras.models.load_model('latinModel', custom_objects={'accuracy_masked':accuracy_masked})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wtokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    word_tokenizer_english = keras.preprocessing.text.tokenizer_from_json(data)\n",
    "    \n",
    "with open('ttokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tag_tokenizer_english = keras.preprocessing.text.tokenizer_from_json(data)\n",
    "    \n",
    "with open('wtokenizerlatin.json') as f:\n",
    "    data = json.load(f)\n",
    "    word_tokenizer_latin = keras.preprocessing.text.tokenizer_from_json(data)\n",
    "    \n",
    "with open('ttokenizerlatin.json') as f:\n",
    "    data = json.load(f)\n",
    "    tag_tokenizer_latin = keras.preprocessing.text.tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________\n",
    "### Step 3: Run the next two cells to prepare the Part-of-Speech-Tagging Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_prediction_english(text):\n",
    "    text = [word_tokenize(text)]\n",
    "    text_encoded = word_tokenizer_english.texts_to_sequences(text) \n",
    "    text_padded = pad_sequences(text_encoded, maxlen=50, padding='pre', truncating='post')\n",
    "    ynew = np.argmax(loaded_english_model_tf.predict(text_padded), axis=-1)\n",
    "    prediction = ynew[0]\n",
    "    prediction = np.trim_zeros(prediction)\n",
    "    decoded = tag_tokenizer_english.sequences_to_texts([prediction])\n",
    "    decoded = word_tokenize(decoded[0])\n",
    "    decoded = [tagdictenglish[tag] for tag in decoded]\n",
    "    print(\"      Sentence= %s\\nPredicted Tags= %s\" % (text, decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_prediction_latin(text):\n",
    "    text = [word_tokenize(text)]\n",
    "    text_encoded = word_tokenizer_latin.texts_to_sequences(text) \n",
    "    text_padded = pad_sequences(text_encoded, maxlen=50, padding='pre', truncating='post')\n",
    "    ynew = np.argmax(loaded_latin_model_tf.predict(text_padded), axis=-1)\n",
    "    prediction = ynew[0]\n",
    "    prediction = np.trim_zeros(prediction)\n",
    "    decoded = tag_tokenizer_latin.sequences_to_texts([prediction])\n",
    "    decoded = word_tokenize(decoded[0])\n",
    "    decoded = [tagdictlatin[tag] for tag in decoded]\n",
    "    print(\"      Sentence= %s\\nPredicted Tags= %s\" % (text, decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "### Step 5: Create your own sentence in the variable named english and run the cell to see its parts of speech. Look up your favorite latin phrase or create your own (if you know latin, that is), store it in the variable named latin, and run the cell to see its parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sentence= [['The', 'red', 'cat', 'took', 'a', 'very', 'long', 'walk', 'along', 'the', 'winding', 'river', '.']]\n",
      "Predicted Tags= ['Pronoun', 'Pronoun', 'Pronoun', 'Pronoun', 'Adposition', 'Pronoun', 'Pronoun', 'Pronoun', 'Pronoun', 'Pronoun', 'Pronoun', 'Pronoun', 'Adposition']\n"
     ]
    }
   ],
   "source": [
    "english = \"The red cat took a very long walk along the winding river.\" \n",
    "output_prediction_english(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sentence= [['Forsan', 'et', 'haec', 'olim', 'meminisse', 'iuvabit', '.']]\n",
      "Predicted Tags= ['Adverb', 'Coordinating Conjunction', 'Adjective', 'Adverb', 'Verb', 'Adjective', 'Punctuation']\n"
     ]
    }
   ],
   "source": [
    "latin = \"Forsan et haec olim meminisse iuvabit.\"\n",
    "output_prediction_latin(latin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
